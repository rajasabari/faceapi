<!DOCTYPE html>
<html>

<head>
  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/volume-meter.js"></script>
  <script src="js/face-api.js"></script>
  <script src="js/faceDetectionControls.js"></script>
  <link rel="stylesheet" href="css/styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
  <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>
  <!-- Minified TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>

  <!-- Minified COCO-SSD -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd/dist/coco-ssd.min.js"></script>
    

</head>
<style>
  #inputVideo {
    width: 150px;
    height: 150px;
    /* background: red; */
    -moz-border-radius: 75px;
    -webkit-border-radius: 75px;
    border-radius: 75px;
    /* height : 220px;
    width: 220px; */
    position: absolute;
    z-index: 9;
    background-color: #f1f1f1;
    text-align: center;
    /* border: 1px solid #d3d3d3; */
    border: 7px solid rgb(252, 217, 20);
  }
  

  
  </style>
<body >

    <!-- <div class="progress" id="loader">
      <div class="indeterminate"></div>
    </div> -->
    <!-- <div id="mydiv" style="position: relative;border: 5px solid red;"  > -->
      
   
    <div style="position: relative;" class="margin">
      <video onloadedmetadata="onPlay(this)" id="inputVideo" autoplay muted></video>
      <span id="overlay" />
    </div>

    <img id="image1" src="http://localhost/pics/img.jpg" alt="Image 1">
    <img id="image2" src="http://localhost/pics/img.jpg" alt="Image 2">

    <input type="file" id="inputImage" accept="image/*" onchange="processImage()">
    <canvas id="outputCanvas"></canvas>
<script>
        
    let forwardTimes = []

    function updateTimeStats(timeInMs) {
      forwardTimes = [timeInMs].concat(forwardTimes).slice(0, 30)
      const avgTimeInMs = forwardTimes.reduce((total, t) => total + t) / forwardTimes.length
      $('#time').val(`${Math.round(avgTimeInMs)} ms`)
      $('#fps').val(`${faceapi.round(1000 / avgTimeInMs)}`)
    }

    // // Load Face API models and initialize Face API
    // Promise.all([
    //   faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),
    //   faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
    //   faceapi.nets.faceLandmark68Net.loadFromUri('/models')
    // ]).then(startComparison);

    // // Perform face comparison
    // async function startComparison() {
    //   const imageURL1 = 'http://localhost/pics/img.jpg';
    //   const imageURL2 = 'http://localhost/pics/img.jpg';

    //   // Load the images
    //   const img1 = await faceapi.fetchImage(imageURL1);
    //   const img2 = await faceapi.fetchImage(imageURL2);

    //   // Detect faces in the images
    //   const face1 = await faceapi.detectSingleFace(img1).withFaceLandmarks();
    //   const face2 = await faceapi.detectSingleFace(img2).withFaceLandmarks();

    //   // Compute feature vectors for the faces
    //   const featureVector1 = await faceapi.computeFaceDescriptor(img1, face1);
    //   const featureVector2 = await faceapi.computeFaceDescriptor(img2, face2);

    //   // Calculate Euclidean distance between the feature vectors
    //   const distance = faceapi.euclideanDistance(featureVector1, featureVector2);

    //   // Display the distance in the console
    //   console.log('Euclidean distance:', distance);
    // }
    
    Promise.all([
      faceapi.nets.ssdMobilenetv1.loadFromUri('models'),
      faceapi.nets.faceLandmark68Net.loadFromUri('models'),
      faceapi.nets.tinyFaceDetector.loadFromUri('models'), // Corrected model name
      faceapi.nets.faceLandmark68Net.loadFromUri('models')
      // faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),
      // faceapi.nets.faceRecognitionNet.loadFromUri('/models')
    ]).then(startDetection);

    // Perform face detection
    async function startDetection() {
      const imageURL = 'http://localhost/pics/img.jpg';

      // Load the image
      const img = await faceapi.fetchImage(imageURL);

      // Detect faces in the image
      const detections = await faceapi.detectAllFaces(img).withFaceLandmarks();

      // Display the results
      if (detections.length > 0) {
        console.log('Faces detected:', detections.length);
        detections.forEach((detection, i) => {
          console.log(`Face ${i + 1}:`, detection);
        });

        const leftEye = detections[0].landmarks.getLeftEye();
        const rightEye = detections[0].landmarks.getRightEye();

        // Calculate the center of the face based on the eyes
        const centerX = (leftEye[0]._x + rightEye[3]._x) / 2;

        let direction = "Facing Forward";
        if (centerX < 0.4) {
            direction = "Facing Left";
        } else if (centerX > 0.6) {
            direction = "Facing Right";
        }

        console.log(direction)

      } else {
        console.log('No faces detected.');
      }
    }

    async function onPlay() {
      const videoEl = $('#inputVideo').get(0)

      if(videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())
      {
        return setTimeout(() => onPlay())
      }

      const options = new faceapi.TinyFaceDetectorOptions()

      const ts = Date.now()

      const result = await faceapi.detectAllFaces(videoEl,options )
      
      // Assuming you have already loaded the required models from the Face API
      
      // Detect faces and analyze facial landmarks
      await faceapi.detectAllFaces(videoEl,options ).withFaceLandmarks().then((facesWithLandmarks) => {
          if (facesWithLandmarks.length === 0) {
            console.log('No faces detected in the image.');
            return;
          }
          
          console.log('Detected faces:', facesWithLandmarks.length);
          
          // Iterate over each detected face
          facesWithLandmarks.forEach((faceWithLandmarks, i) => {
            const landmarks = faceWithLandmarks.landmarks;

            // Assuming you want to detect eyeglasses, check the position of relevant landmarks
            // For example, you can check the position of the nose bridge and eyes
            const noseBridge = landmarks.getNose()[3]; // Index 3 corresponds to the position on the nose bridge
            const leftEye = landmarks.getLeftEye();
            const rightEye = landmarks.getRightEye();

            // Check if the distance between the eyes is relatively wide (indicating eyeglasses)
            const eyeDistance = faceapi.euclideanDistance(leftEye[3], rightEye[0]);
            const noseToEyeDistance = faceapi.euclideanDistance(noseBridge, leftEye[3]);

            // You can define a threshold based on your data to determine eyeglasses presence
            const eyeglassesThreshold = 1.5; // Adjust as needed
            const hasEyeglasses = (eyeDistance / noseToEyeDistance) > eyeglassesThreshold;

            console.log(`Face ${i + 1}: Has Eyeglasses:`, hasEyeglasses);
            
            // Calculate the center of the face based on the eyes
            const centerX = (leftEye[0]._x + rightEye[3]._x) / 2;

            let direction = "Facing Forward";
            if (centerX < 0.4) {
                direction = "Facing Left";
            } else if (centerX > 0.6) {
                direction = "Facing Right";
            }

            console.log(direction)
          });
      })
      .catch((error) => {
        console.error('Error detecting faces and landmarks:', error);
      });


      // startComparison()
      updateTimeStats(Date.now() - ts)

      if (result.length == 1) {
        $("#inputVideo").css("border-color", "green");

        const canvas = $('#overlay').get(0)
        const dims = faceapi.matchDimensions(canvas, videoEl, true)
        //console.log(dims)
        //startSpeech()
        
        // faceapi.draw.drawDetections(span, faceapi.resizeResults(result, dims))
      }else{

        //face not detected block
        // console.log("test");
        $("#inputVideo").css("border-color", "red");
      }

      setTimeout(() => onPlay())
    }

    async function run() {
      // load face detection model
      await changeFaceDetector(TINY_FACE_DETECTOR)
      changeInputSize(128)

      // try to access users webcam and stream the images
      // to the video element
      const stream = await navigator.mediaDevices.getUserMedia({ video: {} })
      const videoEl = $('#inputVideo').get(0)
      videoEl.srcObject = stream
    }

    function updateResults() {}

    $(document).ready(function() {
      //renderNavBar('#navbar', 'webcam_face_detection')
      initFaceDetectionControls()
      run()
    })

    dragElement(document.getElementById("inputVideo"));

function dragElement(elmnt) {
  var pos1 = 0, pos2 = 0, pos3 = 0, pos4 = 0;
  if (document.getElementById(elmnt.id + "header")) {
    /* if present, the header is where you move the DIV from:*/
    document.getElementById(elmnt.id + "header").onmousedown = dragMouseDown;
  } else {
    /* otherwise, move the DIV from anywhere inside the DIV:*/
    elmnt.onmousedown = dragMouseDown;
  }

  function dragMouseDown(e) {
    e = e || window.event;
    e.preventDefault();
    // get the mouse cursor position at startup:
    pos3 = e.clientX;
    pos4 = e.clientY;
    document.onmouseup = closeDragElement;
    // call a function whenever the cursor moves:
    document.onmousemove = elementDrag;
  }

  function elementDrag(e) {
    e = e || window.event;
    e.preventDefault();
    // calculate the new cursor position:
    pos1 = pos3 - e.clientX;
    pos2 = pos4 - e.clientY;
    pos3 = e.clientX;
    pos4 = e.clientY;
    // set the element's new position:
    elmnt.style.top = (elmnt.offsetTop - pos2) + "px";
    elmnt.style.left = (elmnt.offsetLeft - pos1) + "px";
  }

  function closeDragElement() {
    /* stop moving when mouse button is released:*/
    document.onmouseup = null;
    document.onmousemove = null;
  }
}

//audio

var meter = null;
	var WIDTH = 500;
	var recordingStarted = false;

	// initialize SpeechRecognition object
	let recognition = new webkitSpeechRecognition();
	recognition.maxAlternatives = 1;
	recognition.continuous = true;

	// Detect the said words
	// // recognition.onresult = e => {

	// //   	var current = event.resultIndex;

	// //   	// Get a transcript of what was said.
	// //   	var transcript = event.results[current][0].transcript;

	// //   	// Add the current transcript with existing said values
	// //   	var noteContent = $('#saidwords').val();
	// //   	noteContent += ' ' + transcript;
	// //   	$('#saidwords').val(noteContent);

	// // }

	// Stop recording
	// function stopSpeech(){

	//   	// Change status
	//   	$('#status').text('Recording Stopped.');
	//   	recordingStarted = false;

	//   	// Stop recognition
	//   	recognition.stop();
	// }

	// Start recording
	function startSpeech(){
	  	try{ // calling it twice will throw..
	    	// $('#status').text('Noice Level.'); 
	    	// $('#saidwords').val('');
	    	recordingStarted = true;

	    	// Start recognition
	    	// recognition.start();
	  	}
	  	catch(e){}
	}

	navigator.getUserMedia({audio: true}, startUserMedia, function(e) {
	  __log('No live audio input: ' + e);
	});

	function startUserMedia(stream) {
	  	const ctx = new AudioContext();
	  	const analyser = ctx.createAnalyser();
	  	const streamNode = ctx.createMediaStreamSource(stream);
	  	streamNode.connect(analyser);

	  	// Create a new volume meter and connect it.
	  	meter = createAudioMeter(ctx);
	  	streamNode.connect(meter);

	  	drawLoop();

	}

	// Create pitch bar
	function drawLoop( time ) {

	  	var pitchVolume = meter.volume*WIDTH*1.4;

		  var width = 0;
		  var color;

	  	// Pitch detection minimum volume
	  	var minimum_volume = -1;

	  	// Get width if Recording started
	  	if(recordingStarted){

	      if(pitchVolume >= (minimum_volume+80)){console.log(pitchVolume);
				  //  width = 50;
				   color = 'red';
	    	}

	  	}

	  	// Update width
		  // document.getElementById('voiceVolume').style.width = width+'%';
      // document.getElementById('voiceVolume').style.background = color;
      $("#inputVideo").css("border-color", color);
		   
// console.log(color);
	  	rafID = window.requestAnimationFrame( drawLoop );
	}

  const processImage = (function () {
    async function startComparisonInternal() {
      // Your code that uses TensorFlow.js and COCO-SSD can go here
      const inputElement = document.getElementById('inputImage');
        const outputCanvas = document.getElementById('outputCanvas');
        const ctx = outputCanvas.getContext('2d');

        if (inputElement.files.length > 0) {
            const file = inputElement.files[0];

            // Load the selected image
            const img = new Image();
            img.src = URL.createObjectURL(file);

            img.onload = async function () {
                outputCanvas.width = img.width;
                outputCanvas.height = img.height;
                ctx.drawImage(img, 0, 0);

                try{
                  // Load the COCO-SSD model
                  const model = await cocoSsd.load();

                  // Detect objects in the image
                  const predictions = await model.detect(img);

                  // Draw bounding boxes around detected objects
                  for (const prediction of predictions) {
                      ctx.beginPath();
                      ctx.rect(prediction.bbox[0], prediction.bbox[1], prediction.bbox[2], prediction.bbox[3]);
                      ctx.lineWidth = 2;
                      ctx.strokeStyle = 'red';
                      ctx.fillStyle = 'red';
                      ctx.stroke();console.log(`${prediction.class}`);
                      ctx.fillText(`${prediction.class} (${Math.round(prediction.score * 100)}%)`, prediction.bbox[0], prediction.bbox[1] > 10 ? prediction.bbox[1] - 5 : 10);
                  }
                }
                catch(err){
                  console.error(err);
                }
            };
        }
    }

    // Return the function so that it can be accessed from outside
    return startComparisonInternal;
  })();


  
  </script>
</body>
</html>