// test('tokenize() accepts lexicon param', function (t) {
//   let doc = nlp.tokenize('spencer kelly is working here', {
//     'spencer kelly': 'Person',
//     working: 'NotFun',
//   })
//   t.equal(doc.match('#Person+').text(), 'spencer kelly', here + 'used tag')
//   t.equal(doc.match('#NotFun').text(), 'working', here + 'used 2nd tag')
//   t.equal(doc.has('#Verb'), false, here + 'not a full tag')
//   t.end()
// })
